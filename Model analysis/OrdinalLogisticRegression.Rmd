---
title: "Ordinal Logistic Regression"
author: Jeremy and Peter Sykora
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(foreign)
library(MASS)
library(Hmisc)
library(reshape2)
library(tidyverse)
library(dplyr)
```

### Step 1: Reading in the Data

```{r}

# read in the original, unmodified data set
df = read_csv("student-mat.txt")

# read in the recoded data set where the variables have values that are recoded from 1 to 5 to more descriptive categories and with a new variable related to `Walc` that is `Walc_bin` with values "yes" meaning that the student consumes alcohol and "no" meaning that the student does not consume alcohol.
df_recoded_binary = read_csv("student-mat-recoded-binary.txt")

# modify the recoded data set
df_recoded_binary <- df_recoded_binary %>%
  mutate(Walc = as.ordered(df$Walc), # convert our dependent variable from type double to type ordinal factor with 5 levels
         sex = as.factor(sex), # convert all non-continuous variables to categorical variables
         school = as.factor(school),
         address = as.factor(address),
         famsize = as.factor(famsize),
         Pstatus = as.factor(Pstatus),
         Medu = as.factor(Medu),
         Fedu = as.factor(Fedu),
         Mjob = as.factor(Mjob),
         Fjob = as.factor(Fjob),
         reason = as.factor(reason),
         guardian = as.factor(guardian),
         traveltime = as.factor(traveltime),
         studytime = as.factor(studytime),
         schoolsup = as.factor(schoolsup),
         famsup = as.factor(famsup),
         paid = as.factor(paid),
         activities = as.factor(activities),
         nursery = as.factor(nursery),
         higher = as.factor(higher),
         internet = as.factor(internet),
         romantic = as.factor(romantic),
         famrel = as.factor(famrel),
         freetime = as.factor(freetime),
         Dalc = as.factor(Dalc),
         goout = as.factor(goout),
         health = as.factor(health),
         Walc_bin = as.factor(Walc_bin)
  ) 

# let us now examine the values and data types
head(df_recoded_binary)
str(df_recoded_binary)

```

```{r}

# let us also examine a summary of the modified recoded data set
summary(df_recoded_binary)

```

### Step 2: Create the Ordinal Logistic Regression Model

Here, we will first fit an ordinal logistic regression on all of the predictors.

```{r}

# we fit a model with every predictor besides `Walc_bin` because `Walc_bin` is the binary representation of `Walc`, our response variable
model = polr(Walc ~ . - Walc_bin, data = df_recoded_binary, Hess = TRUE)

# let us examine the summary statistics of the fitted model
summary(model)

```

### Step 3: Obtaining P-Values

Now, we will determine the predictors' levels of significance from the previously fitted model.

```{r}

ctable = coef(summary(model))
p = pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable = cbind(ctable, "p value" = p))

```

### Step 4: Eliminate Statistically Insignificant Variables & Arrive At Final Model

* Significance level at alpha = 0.10

* For categorical predictors, if any of the subcategories (e.g Fjobother) is significant, then we will keep the independent variable regardless of the significance of the other subcategories within the IV

* Dalc is significant, but it would seem unfair to have weekday alcohol consumption as a predictor for weekend alcohol consumption. If we have access to a student's weekday alcohol consumption, then it would be reasonable to also have access to a student's weekend alcohol consumption, making the model redundant. Therefore, it is not included in the model as a predictor.

#### Final Model

```{r}

model = polr(Walc ~ .-Walc_bin-school-sex-age-address-famsize-Pstatus-Mjob-reason-failures-
               schoolsup-activities-higher-internet-romantic-famrel-G1-G2-G3-famsup-Dalc-freetime, data = df_recoded_binary, Hess=TRUE)
ctable = coef(summary(model))
p = pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable = cbind(ctable, "p value" = p))

```

### Step 5: Evaluate the Model by Calculating the Error

```{r}

pred = predict(model, df_recoded_binary)
pred

```

```{r}

(tab = table(pred, df_recoded_binary$Walc))
1-sum(diag(tab)) / sum(tab)

```

Using the entire dataset as the training set, and evaluating the model on this the entire dataset, the model only predicted 45% of the weekend alcohol consumption correctly. This is pretty bad.

### Step 6: Testing For Overfitted Model

Let us now consider using 80% of the dataset as the training set and then evaluating the model on the remaining 20% of the entire dataset. This will help us determine if the previous model fit is overfitted.

```{r}

# randomly shuffle the recoded data set
df_recoded_binary_shuffled <- df_recoded_binary[sample(nrow(df_recoded_binary), replace = FALSE),]

# extract the first 80% of the total rows from the shuffled data set
df_recoded_binary_80 <- df_recoded_binary_shuffled %>% slice(1:(nrow(df_recoded_binary)*0.8))

# extract the remaining 20% of the total rows from the shuffled data sete
df_recoded_binary_20 <- df_recoded_binary_shuffled %>% slice((nrow(df_recoded_binary)*0.8)+1:(nrow(df_recoded_binary)*0.2))

```

From the model that we fit above using all of the predictors, we have already found the predictors that are significant in predicting `Walc`. Thus, we simply have to fit a model using these predictors and then examine the model's prediction accuracy on the remaining 20% of the shuffled data set.

```{r}

# we fit a model with every predictor besides `Walc_bin` and the predictors that were found to be insignificant in the previous model fitted with all of the predictors
model <- polr(Walc ~ . -Walc_bin-school-sex-age-address-famsize-Pstatus-Mjob-reason-failures-
               schoolsup-activities-higher-internet-romantic-famrel-G1-G2-G3-famsup-Dalc-freetime, data = df_recoded_binary_80, Hess=TRUE)

ctable = coef(summary(model))
p = pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
ctable = cbind(ctable, "p value" = p)

pred_80 <- predict(model, df_recoded_binary_80)
# pred_80
(tab_train <- table(pred_80, df_recoded_binary_80$Walc))

pred_20 <- predict(model, df_recoded_binary_20)
# pred_80
(tab_test <- table(pred_20, df_recoded_binary_20$Walc))

```
## Loss Function #1: Proportion of Correct Predictions

This loss function only cares about the proportion of predictions that match to the exact label. It does not consider the distance a prediction is to the correct label.


Training Set:
```{r}
zeroOne = function(answer, pred){
  tab =  table(pred, answer)
  return(sum(diag(tab))/sum(tab))
}

pred <- predict(model, df_recoded_binary_80)
answer = df_recoded_binary_80$Walc
zeroOne(answer, pred)
```
Test Set:
```{r}
pred <- predict(model, df_recoded_binary_20)
answer = df_recoded_binary_20$Walc
zeroOne(answer, pred)
```

## Loss Function #2: Mean Absolute Deviation from Correct Label
```{r}
abs_deviation <- function(answer, pred){
  ## answer and pred need to contain numerical values
  absolute_diff <- abs(answer-pred)
  mean_deviation <- mean(absolute_diff)
  return(mean_deviation)
}
```

Training Set
```{r}
pred = as.double(pred_80)
answer = as.double(df_recoded_binary_80$Walc)

abs_deviation(answer, pred)
```
Test Set:
```{r}
pred = as.double(pred_20)
answer = as.double(df_recoded_binary_20$Walc)

abs_deviation(answer, pred)
```


## Loss Function #3: Mean Squared Deviation from Correct Label

```{r}
square_deviation <- function(answer, pred){
  ## answer and pred need to contain numerical values
  square_diff <- (answer-pred)**2
  mean_deviation <- mean(square_diff)
  return(mean_deviation)
}
```

Training Set
```{r}
pred = as.double(pred_80)
answer = as.double(df_recoded_binary_80$Walc)

square_deviation(answer, pred)
```
Test Set
```{r}
pred = as.double(pred_20)
answer = as.double(df_recoded_binary_20$Walc)

square_deviation(answer, pred)
```
### Testing Different Variable Combinations

## Combination 1: Logit Link

```{r}
model <- polr(Walc ~ internet+higher+G2+G3+Pstatus+school+Fedu, data = df_recoded_binary_80, Hess=TRUE)

```


```{r}
model <- polr(Walc ~ sex+goout+G3+absences+G2+studytime+age+Fjob+failures+G1+famsup+romantic+reason, data = df_recoded_binary_80, Hess=TRUE)
```


```{r}
pred_80 <- predict(model, df_recoded_binary_80)
# pred_80
(tab_train <- table(pred_80, df_recoded_binary_80$Walc))

pred_20 <- predict(model, df_recoded_binary_20)
# pred_80
(tab_test <- table(pred_20, df_recoded_binary_20$Walc))
```


